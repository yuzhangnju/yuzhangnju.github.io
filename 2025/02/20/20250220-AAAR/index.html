

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://cdn.jsdelivr.net/gh/yuzhangnju/image2024/uchiha.png">
  <link rel="icon" href="https://cdn.jsdelivr.net/gh/yuzhangnju/image2024/uchiha.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Yu Zhang">
  <meta name="keywords" content="HIBL, AI, AAAR">
  
    <meta name="description" content="一个社会科学博士生的AI使用经验。">
<meta property="og:type" content="article">
<meta property="og:title" content="AAAR-使用AI加速学术研究的方法论与经验">
<meta property="og:url" content="https://yuzhang.net/2025/02/20/20250220-AAAR/index.html">
<meta property="og:site_name" content="Yu&#39;s Space">
<meta property="og:description" content="一个社会科学博士生的AI使用经验。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/ai-in-academic-research-blog%20(1).png">
<meta property="article:published_time" content="2025-02-20T10:00:00.000Z">
<meta property="article:modified_time" content="2025-02-20T14:07:07.626Z">
<meta property="article:author" content="Yu Zhang">
<meta property="article:tag" content="HIBL">
<meta property="article:tag" content="AAAR">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/ai-in-academic-research-blog%20(1).png">
  
  
  
  <title>AAAR-使用AI加速学术研究的方法论与经验 - Yu&#39;s Space</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"yuzhang.net","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":100,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":false},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Yu&#39;s Space</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://cdn.jsdelivr.net/gh/yuzhangnju/image/img/bannernew.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="AAAR-使用AI加速学术研究的方法论与经验"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-02-20 18:00" pubdate>
          2025年2月20日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          11k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          89 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">AAAR-使用AI加速学术研究的方法论与经验</h1>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p>在这篇文章里，我从一个<strong>社会科学</strong>博士生的视角出发，详细介绍了自己在日常科研中对于<strong>AI</strong>的使用经验与体会。</p>
<p>我将AI在社科中的应用（与影响）分为了<strong>5个层次</strong>，分别是：<strong>RA
level</strong>、<strong>Supervisor level</strong>、<strong>Domain expert
level</strong>、<strong>Agent level</strong>和<strong>Governance
level</strong>。我会结合案例与文献，有重点地展示这5个层次的使用，希望达到一个“良好实践”的效果（尽管可能不是最佳实践）。</p>
<p>如果本文对您有所帮助，还请您不吝在页面尾部对我的工作进行“<strong>赞赏</strong>”，以帮助我减轻一些大模型使用的昂贵费用，非常感谢！</p>
</blockquote>
<p>本文知识的截止日期为2025年2月20日。</p>
<h2 id="引言">引言</h2>
<p>这篇推文的标题是<strong>AAAR</strong>，取自<em>AI Accelerating
Academic
Research</em>的首字母，是仿照人工智能顶会之一的AAAI构造的，可以读作“Triple
AR”。</p>
<p>我一直对前沿科技很感兴趣，日常摸鱼的很多时间也花在这上面。以ChatGPT为代表的大语言模型，很大程度上改变了我的工作流。这篇推文基于我在两个<strong>reading
group</strong>所做的同名pre改写，即将pre的内容整理成文字稿，并更新了一些内容，感谢群里小伙伴们的讨论。</p>
<p>本文中所涉及的案例，在pre时主要依赖现场演示，在推文中，我很难仅靠文字和截图进行完全展示，因此在效果上会有一些折扣。另外，这篇推文的案例主要涉及日常工作流和部分科研方向，并非最前沿的AI研究进展or应用，它是接地气的、<strong>可操作性强</strong>的。</p>
<p>本文结构如下：首先讨论了为什么要使用AI，接下来讲解了生成式AI的一些特点和使用技巧，提供一些简明的挑选大模型的参考资料，第三部分是本文的重点，讲述了我对AI在社科领域应用的层次划分，第四部分是案例展示和一些便捷的AI科研工具介绍。</p>
<h2 id="为什么要使用ai">为什么要使用AI</h2>
<h4 id="ai的iphone时刻"><strong>AI的iPhone时刻</strong></h4>
<p>为什么要使用AI，这听起来像个无厘头的问题。这个问题其实主要是面向一些<strong>保守人士</strong>和<strong>中老年科研工作者</strong>说的。在ChatGPT刚出来的时候，有些上了年纪的老师评论“<strong>不过如此</strong>”，不会让他丢饭碗。结果今年春节deepseek
r1爆火，同一批老师惊叹r1的写作能力、科研设计的能力比自己更强，并表示要将deepseek
r1纳入自己的工作流。</p>
<p>一个合理的推论是，这些老师没有跟踪大语言模型在这两年的飞速进展，他可能直接从ChatGPT
3.5跳到了deepseek r1。</p>
<p>我越来越认同人是有<strong>局限性</strong>的，这种局限性是很难摆脱的。10年前我高考毕业在OPPO专卖店卖手机时，一位中年顾客问我为什么刚拿出来的新手机需要更新系统，我说这是为了完善功能，而他反问我这不是拿功能不完善的产品出来销售吗？确实，以前的功能机大多不需要更新系统，出厂即终身。</p>
<p>在2024年英伟达GTC大会上，老黄穿插讲了许多次“<em>We are at the iPhone
moment of
AI</em>”，这是一个最前沿从业者的判断。iPhone开启了移动互联网时代，其变革性无需多言。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250219200913932.png" srcset="/img/loading.gif" lazyload /></p>
<p>与保守相对应的是一种<strong>短视</strong>，即“人们总是会高估技术的短期影响，又会低估技术的长期影响”。</p>
<p>过年两年里AI有诸多进展，每一次的产品迭代都让我非常振奋，就像回到了2010年的搞机时代，惊艳于新品的功能与设计。硅谷的互联网巨头们仿佛一下子年轻了起来，干劲十足。从纯文本的聊天机器人，到多模态大模型，再到实时音视频模型、最近半年的推理模型，以及最新的deep
research，AI已经从跃升到了agent层次。AI带来的长期影响，早已在创新的前沿埋下了种子。</p>
<p>尽管有人认为生成式 AI
可能是泡沫，但它对我们学术圈、尤其是年轻研究者而言，的确正在带来翻天覆地的变化。它不仅仅能当一个“<strong>锤子</strong>”去加速已有流程，更重要的是，它开始展现出一定程度的“<strong>推理</strong>”能力。即便有人评价这只是一种基于海量语料“预测下一个词”的随机鹦鹉，但你我又如何能确定，我们自身不是一个复杂的函数拟合器呢？所以，不妨试试吧，将AI纳入自己的工作流。</p>
<h4 id="ai是优质的ra"><strong>AI是优质的RA</strong></h4>
<p>在经济学领域，使用AI有个非常现实的好处就是<strong>省钱</strong>。</p>
<p>econ的市场已经充满毒性，现在<strong>predoc</strong>非常普遍，RA们拿着微薄的酬劳，干着扒古籍数据、抄地方志等脏活、苦活、累活。在小红书上，能搜到很多老师招predoc的帖子。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250219204502718.png" srcset="/img/loading.gif" lazyload /></p>
<p>假设以前一个老师的工作需要招10个RA，每人每月给200美元，现在，最前沿的AI（GPT
Pro）订阅费用也是200美元，老师完全可以只招3个RA，然后给每个人都开GPT
Pro或给等额的API费用，这样，3个真人RA在AI的加持下，工作产出应该可以远超10个传统RA，而且RA跑路的风险进一步降低。</p>
<h2 id="生成式ai的特点与使用基础">生成式AI的特点与使用基础</h2>
<h4 id="gen-ai与传统技术的区别"><strong>Gen
AI与传统技术的区别</strong></h4>
<p>我不知道大家有没有想过这一轮生成式AI与传统机器学习技术的最大不同点是什么，这里传统机器学习技术指基于SVM、决策树、回归等算法的<strong>分类器</strong>或其他应用场景。</p>
<p>一个形象的比喻是，传统的机器学习方法像是一把需要手动锻造的“<strong>锤子</strong>或“<strong>筛子</strong>”，以监督学习为例，它的使用场景通常如下：</p>
<ul>
<li>首先，选择合适的经典算法；</li>
<li>其次，在数据集中划分20%作为<strong>训练集</strong>和<strong>验证集</strong>，找一批本科生或比较乖的硕士生来给数据<strong>打标签</strong>，比如微博评论or人民网留言板；</li>
<li>最后，构造出锤子或筛子，用来<strong>批量</strong>处理剩下的数据，筛子上有不同形状的孔洞，数据放进去，就会归到对应的孔里，完成分类工作。这样，你就有了想要的x或者y。</li>
</ul>
<p>这样的锤子或筛子，有着一定的<strong>局限</strong>：</p>
<ul>
<li>需要人工打标签，这需要一定的时间和金钱；</li>
<li>模型的性能可能不会很好；</li>
<li>最重要的，这样的锤子或筛子，换到新的数据集或场景，就需要重新打造。</li>
</ul>
<p>而像ChatGPT这样的大语言模型，恰好可以规避上述局限。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250219205515693.png" srcset="/img/loading.gif" lazyload /></p>
<p>上面这张图的右侧是<strong>布鲁姆的教育目标分类模型</strong>，将认知学习过程分为六个层级，从基础的知识<strong>记忆</strong>到高级的<strong>创造</strong>性思维，强调学习的层次递进关系。按这个模型来比较，生成式AI已经刷通了六个层次。如果你是一位老师来做图灵测试，能判断出答案背后是自己的学生还是AI吗？</p>
<p>总而言之，我认为生成式AI借助语言文字，具备了人的特征，并且其知识广度吊打地球上任何一个人，Hinton老爷子认为这是反向传播算法相对于人脑神经元的优势。</p>
<h4 id="gen-ai使用的简明教程"><strong>Gen AI使用的简明教程</strong></h4>
<p>生成式AI的能力取决于两方面，即<strong>模型能力</strong>和是否使用合适的<strong>提示词</strong>，随着模型能力的强化，提示词的重要性越来越低。</p>
<p><strong>选择模型</strong></p>
<p>选择合适的模型至关重要，在这里，我推荐大家使用<strong>大模型竞技场</strong>作为参考，网址是：https://lmarena.ai/?leaderboard</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250219214613773.png" srcset="/img/loading.gif" lazyload /></p>
<p>⬆️以上是大模型竞技场在2025年2月19日的部分排名情况</p>
<p>目前排名最高的是刚刚发布的grok
3，但还没有公开可用。谷歌的Gemini系列模型表现亮眼，如果你没有开通ChatGPT
plus或者pro，那么在<strong>Google AI
studio</strong>里使用免费的Gemini系列模型是个不错的选择。这个表格里<strong>缺失</strong>了OpenAI的前沿模型，比如o1pro。</p>
<p>下面是我<strong>主观</strong>上对各个模型能力的排名和特点介绍：</p>
<ul>
<li><strong>ChatGPT
o1pro</strong>：目前为止最强的推理模型，拥有很长的上下文窗口，能输出很长的文本。在科学论文写作方面，逻辑性强，AI味很低，是我使用最多，最深度依赖的模型。当然成本也很高，需要订阅Pro会员，费用是200美元/月；</li>
<li><strong>ChatGPT deep
research</strong>：OpenAI刚发布不久的研究型智能体，Pro会员每月可以使用100次，是目前为止最强大的科研agent，它在撰写研究报告、论文综述方面能力出色，甚至可以一次性生成6万字的报告，是最近让我思考做科研的意义的模型。但由于它被设计为一个科研agent，所以它不擅长日常任务，不管你的prompt如何写，它还是会主观根据它的能力去网络上查找数据资料，试图生成一个报告或综述。我非常期待其底座模型o3早日上线，据信其会作为GPT5的高性能模式推出；</li>
<li><strong>Gemini 2.0 Flash Thinking Experimental
01-21</strong>：我在处理长文本任务和需要即时反馈任务时常常会使用的模型，其支持1M的上下文窗口，意味着你可以一次性丢几十篇论文进去。o1pro是个<strong>重推理</strong>模型，一个简单的问题可能也需要思考几分钟，而Gemini
2.0 Flash Thinking
Experimental通常可以在十几秒内开始应答，比如翻译句子、润色文本这类任务，就可以交给它；</li>
<li><strong>ChatGPT o3mini high</strong> 和 <strong>Claude 3.5
sonnet</strong>：适合用来写代码，又快又好；</li>
<li><strong>DeepSeek
R1</strong>：适合用来搞玄学、写<strong>文科黑话</strong>、角色扮演谈恋爱、写散文，总之不太适合逻辑性强、真实性要求高的科研任务。它的模型<strong>幻觉</strong>太严重，调低模型的温度参数会有改善。去年六月，我就使用过deepseek
v2的低参数本地版本。这次R1的发布，非常震撼。可能是由于模型对齐工作做得比较少，它的AI味非常低、人味很浓，我很喜欢跟它聊天扯淡。由于它太擅长写作文科黑化，可能比较适合现代文学、哲学、新闻学、社会学（注意不是社会科学）方向的人，be
like 张一兵bot。我最近的使用场景如下：<img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220081833134.png" srcset="/img/loading.gif" lazyload /></li>
</ul>
<p><strong>提示词框架</strong></p>
<p>网络上流传着各种提示词框架，大体结构是一致的，目的都是向AI表明你的需求。以下简单介绍一下<strong>CO-STAR</strong>提示词框架：</p>
<ul>
<li><strong>Context</strong>：背景/场景。为模型提供必要的背景信息，让它理解你的请求的语境；</li>
<li><strong>Objective</strong>：目标。明确你希望模型达成的目标，你想让模型做什么？你的最终目的是什么？</li>
<li><strong>Style</strong>：风格。指定你希望模型以何种风格进行回复，例如正式、非正式、幽默、专业、像专家一样、像朋友一样、像诗人一样等等；</li>
<li><strong>Tone</strong>：语气。设定你想要的语气，例如积极、消极、客观、主观、幽默的等等；</li>
<li><strong>Audience</strong>：受众/人群。明确你的目标受众是谁，模型需要根据受众的特点调整回复，例如儿童、专家、普通用户等等；</li>
<li><strong>Response
Format</strong>：回复格式/形式。指定你希望模型以何种格式回复，例如列表、段落、代码、表格、总结等等。</li>
</ul>
<p>总之，有意识地告诉AI一个清晰、可执行的目标和输出要求即可。</p>
<p>对于特别想要优化的提示词，在开始阶段，可以使用一些在线的提示词优化工具，比如：</p>
<ul>
<li>Kimi提示词专家：https://kimi.moonshot.cn/kimiplus/conpg00t7lagbbsfqkq0</li>
<li>AI提示生成器：https://www.aipromptgenerator.net/zh</li>
</ul>
<p>如果你经常有翻译或润色任务，可以将相关指令设置为模型的个性化提示词。我整理了一些常用的翻译、润色提示词，参见我博客的置顶内容（https://yuzhang.net/）。</p>
<p>另外有uu提到同样的问题，使用英文prompt得到的回复质量更高，大家也可以试试。</p>
<h2 id="ai-for-social-sciences-的5个层次">AI for Social Sciences
的5个层次</h2>
<p>我将AI在社会科学中的应用（或影响）分成了下面5个层次，分别对应了不同级别的任务或视野：
<img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220082825623.png" srcset="/img/loading.gif" lazyload /></p>
<p>⬆️以上Level1-5的标题，看起来像文科黑话，是我把自己的分类想法告诉GPT，再由它提炼出来的。</p>
<h4 id="level-1---方法论与技术工具的扩展ra-level"><strong>level 1 -
方法论与技术工具的扩展（RA level）</strong></h4>
<p>这是最直接的层面，AI作为新的<strong>工具箱</strong>，集成了很多好用的锤子、筛子、刷子，扩充了社会科学的方法论手段。研究者可以使用零样本学习的文本分类、从非结构化数据提取信息、自动化写作辅助等。这一层面，AI相当于一位超级“<strong>研究助理</strong>”，帮助完成繁琐或困难的任务。例如，在日常工作中，我常把AI用于初步数据清洗、格式转换、文献搜集，以及简单的写作润色等。这些都属于工具层面的增强，大幅提升效率。但核心研究思路仍由人主导，理论框架也基本不变。</p>
<h4 id="level-2---认知与知识生产的扩容supervisor-level"><strong>level 2
- 认知与知识生产的扩容（Supervisor level）</strong></h4>
<p>在这一层，AI不仅是工具，更影响了知识生产的<strong>认知过程</strong>，是一位不眠不休没有脾气不会PUA你的<strong>导师</strong>。比如，AI可以参与<strong>idea讨论</strong>、优化研究设计，以及引入多学科知识互动。当ChatGPT能够和研究者讨论问题时，知识的产生变成了一种<strong>对话式</strong>过程。研究者可能会向AI征询灵感、验证想法，从而突破个人知识的局限。这种交互为学术创造力提供了新源泉。我在自己的体验中就多次因为与AI的对话而闪现新思路。此外，多学科知识整合方面，AI庞大的训练语料涵盖各种学科，如果善加利用，相当于随时请教各领域入门知识。这种知识生产方式的扩容，潜移默化地改变了研究的<strong>认知链条</strong>：过去可能是<strong>人-&gt;文献-&gt;人自行综合</strong>，现在是<strong>人-&gt;
AI提示下获取综合知识-&gt; 再生产想法</strong>。</p>
<h4
id="level-3---计算理性与推理深度的提升domain-expert-level"><strong>level
3 - 计算理性与推理深度的提升（Domain expert level）</strong></h4>
<p>这里，我用了Domain
expert的词汇，表明这部分任务在传统上可能需要专家来做，比如经济学领域擅长写模型的研究人员，或者算法工程师。现在，借助前沿推理模型，我们可以很好地让AI来解释、优化经济学模型。</p>
<p>以我自己为例，我没有什么代码基础，只精通python的下载、安装、配置，顶多再用一下GitHub里现成的轮子。但是借助vscode
copilot或者cursor，以及询问Claude 3.5
sonnet，我能用python完成一些以前需要请人帮忙才能完成的任务。</p>
<h4 id="level-4---ai作为本体论实体的考量agent-level"><strong>level 4 -
AI作为本体论实体的考量（Agent level）</strong></h4>
<p>这一层次的核心是：AI模拟人类。</p>
<p>当AI变得越来越智能，我们不得不在社会科学的<strong>本体论</strong>中考虑AI角色。也就是说，过去我们的研究对象几乎清一色是“人”或人造的组织机构，现在出现了一个新的行动者：AI。社会科学理论开始探讨<strong>AI智能体在社会系统中的地位</strong>。例如，在经济市场上，高频交易算法已经在发挥作用，它们可以看作经济行动者的一种；在政治舆论领域，社交媒体上的bots（自动账户）影响舆情，也是一类“参与者”。这要求理论上对“行动者”概念进行扩展，纳入非人智能体。伦理学和社会学也开始问：AI能否被视为有行为责任的主体？AI若参与决策，如何衡量其决策？我将AI模拟社会实验的案例也归入这一层次，因为当我们用AI来模拟社会时，我们实际上把AI当成了社会中的代理人去考虑。这种<strong>本体论重构</strong>迫使社会科学反思很多基本概念，比如互动不一定发生在人和人之间，也可能在人和AI之间，甚至AI和AI之间（例如自动驾驶汽车在道路上“互动”）。</p>
<h4
id="level-5---制度变迁与价值体系重构的宏观审视governance-level"><strong>level
5 - 制度变迁与价值体系重构的宏观审视（Governance level）</strong></h4>
<p>这一层次不关注AI的应用，而注重宏观影响，涉及<strong>社会结构</strong>和<strong>价值体系</strong>本身如何因AI而改变</p>
<p>当AI大规模应用于社会，各种制度（法律、市场机制、教育体系等）可能随之演进。社会科学需要在宏观上审视AI引发的结构性变化，并参与价值引导。这既包括研究<strong>AI对就业、收入分配、权力结构</strong>的影响，也包括反思<strong>算法治理</strong>、<strong>数字民主</strong>等新模式。一方面，也许我们将看到一些传统制度被AI优化：比如司法领域引入AI辅助判案，提高效率；但另一方面，这也可能改变权力格局，引发对算法公正的质疑。当决策越来越依赖AI时，人们是否会过度信任技术权威而削弱民主参与？价值观层面，AI的崛起让人们重新思考“智能”与“意识”的定义，我们如何看待人类自身的独特性？社会科学在这一层次需要和哲学、伦理学结合，去<strong>规范</strong>AI的发展方向。例如，构建对AI的治理框架、倡导以人为中心的AI设计，确保技术为社会服务而非异化社会。这个层面其实已超出传统学术研究范畴，更像一种<strong>社会参与</strong>。许多社会科学学者开始活跃于AI政策咨询、伦理指南制定等实践活动，为AI时代塑造合理的制度和价值贡献智慧。</p>
<p>在之前准备pre的过程中，我查找了一些论文⬇️，基本可以对应上面5个层次（以序号标出），这些论文的获取详见页尾链接。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220105846066.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="案例展示">案例展示</h2>
<h4 id="case-1-技术工具的拓展ra-level"><strong>Case 1:
技术工具的拓展（RA level）</strong></h4>
<p>这部分是最基础的应用，也是最常见的应用场景。这里我首先介绍一些翻译、写作等任务工具，再举例介绍一个用AI做零样本文本分类的脏活儿。</p>
<p><strong>写作辅助</strong></p>
<p>可以尝试用ChatGPT Canvas或者Claude 3.5
Artifacts，就像在word里插入一下AI写作助手一样，你可以让它帮你优化表达、选中解释的内容、调整长度风格、改变面向人群等。交互界面如下：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220093839472.png" srcset="/img/loading.gif" lazyload /></p>
<p>⬆️比如这里，我先让它优化了一段文字的表达，并让它将表达和文本面向的深度改为研究生层次，效果如下⬇️：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220094008826.png" srcset="/img/loading.gif" lazyload /></p>
<p>Canvas是个蛮好用的工具，可以很方便地查看更改、撤回、复制等⬇️：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220094105909.png" srcset="/img/loading.gif" lazyload /></p>
<p>但它不擅长长文本的修改，改起来会很麻烦，每次都会全量刷新。</p>
<p>Canvas也可以修改和运行代码，您可以试试。</p>
<p><strong>信息提取等苦活累活</strong></p>
<p>以前提取结构化信息，总需要自己写很多代码，或者用云服务商的套件，现在可以尝试调用API或者本地模型，大力出奇迹。以下是两篇介绍，有需要的小伙伴请自行查看：</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/7EdEEwW2RlZSeMKYspIIKA">利用LLM从科学文本中抽取结构化信息</a></li>
<li><a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/QSsDpfyJmGRR6IVdlvWdwg">使用本地大模型从文本中提取结构化信息</a></li>
</ul>
<p><strong>零样本文本分类</strong></p>
<p>这里我重点演示一下传统机器学习方法可能不太好应对的任务，描述如下：<strong>对新闻联播文本进行分类，划分为国内新闻和国外新闻</strong>。这个任务具有以下特征：</p>
<ul>
<li>新闻联播文本是长文本，相对于传统的微博评论等文本，处理难度更高；</li>
<li>如果用传统方法做监督分类，需要打一些标签，而且有些标签不太好打，比如国家领导人会见外宾，容易被判为国外新闻。</li>
</ul>
<p>这个案例用大语言模型就比较容易做到，<strong>大力出奇迹</strong>，思路如下：</p>
<ul>
<li>使用LLM，写好提示词，将新闻联播文本喂进去；</li>
<li>让其输出国际新闻的第一句；</li>
<li>使用python脚本，将大模型输出的国际新闻第一句匹配到原文本里做截断，在此之前的为国内新闻。</li>
</ul>
<p>代码如下（话说公众号文章可以折叠代码吗）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI<br><br><span class="hljs-comment"># 设置日志</span><br>logging.basicConfig(filename=<span class="hljs-string">&#x27;news_processing.log&#x27;</span>, level=logging.DEBUG,<br>                    <span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;%(asctime)s - %(levelname)s - %(message)s&#x27;</span>)<br><br><span class="hljs-comment"># 同时将日志输出到控制台</span><br>console = logging.StreamHandler()<br>console.setLevel(logging.INFO)<br>formatter = logging.Formatter(<span class="hljs-string">&#x27;%(asctime)s - %(levelname)s - %(message)s&#x27;</span>)<br>console.setFormatter(formatter)<br>logging.getLogger(<span class="hljs-string">&#x27;&#x27;</span>).addHandler(console)<br><br><span class="hljs-comment"># 初始化OpenAI客户端</span><br>client = OpenAI(<br>    api_key=os.getenv(<span class="hljs-string">&quot;DASHSCOPE_API_KEY&quot;</span>),<br>    base_url=<span class="hljs-string">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span>,<br>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">qwen_chat</span>(<span class="hljs-params">messages, model=<span class="hljs-string">&quot;qwen-plus-latest&quot;</span>, temperature=<span class="hljs-number">0.7</span>, timeout=<span class="hljs-number">60</span></span>):<br>    <span class="hljs-keyword">try</span>:<br>        completion = client.chat.completions.create(<br>            model=model,<br>            messages=messages,<br>            temperature=temperature,<br>            timeout=timeout<br>        )<br>        <span class="hljs-keyword">return</span> completion.choices[<span class="hljs-number">0</span>].message.content.strip()<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        logging.error(<span class="hljs-string">f&quot;API请求失败：<span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(e)&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">find_split_point</span>(<span class="hljs-params">text</span>):<br>    prompt = <span class="hljs-string">f&quot;&quot;&quot;你是一名中国人，正在阅读一篇完整的新闻联播文本。你的任务是从中国人的视角，找到国际新闻部分的起始句子。</span><br><span class="hljs-string"></span><br><span class="hljs-string">要求：</span><br><span class="hljs-string">1. 从中国人的角度，区分国内新闻和国际新闻。</span><br><span class="hljs-string">2. 国际新闻通常涉及外国的事件、人物、国家或国际组织等。</span><br><span class="hljs-string">3. 在新闻文本中，找到国际新闻部分的第一个完整句子。</span><br><span class="hljs-string"></span><br><span class="hljs-string">请直接返回国际新闻部分的起始句，不要添加任何解释或额外内容。</span><br><span class="hljs-string"></span><br><span class="hljs-string">新闻文本：</span><br><span class="hljs-string"><span class="hljs-subst">&#123;text&#125;</span></span><br><span class="hljs-string"></span><br><span class="hljs-string">请直接返回国际新闻部分的起始句。&quot;&quot;&quot;</span><br>    <br>    messages = [<br>        &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;You are a helpful assistant.&quot;</span>&#125;,<br>        &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: prompt&#125;<br>    ]<br>    <br>    content = qwen_chat(messages, temperature=<span class="hljs-number">0.1</span>, timeout=<span class="hljs-number">300</span>)<br>    <br>    <span class="hljs-keyword">if</span> content <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    logging.debug(<span class="hljs-string">f&quot;API 返回的内容：\n<span class="hljs-subst">&#123;content&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> content<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">split_news_content</span>(<span class="hljs-params">text, split_sentence</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> split_sentence:<br>        logging.error(<span class="hljs-string">&quot;未能识别出分界句&quot;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    split_index = text.find(split_sentence)<br>    <span class="hljs-keyword">if</span> split_index == -<span class="hljs-number">1</span>:<br>        logging.warning(<span class="hljs-string">f&quot;在文本中未找到分界句：<span class="hljs-subst">&#123;split_sentence&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    domestic_content = text[:split_index].strip()<br>    foreign_content = text[split_index:].strip()<br><br>    logging.info(<span class="hljs-string">f&quot;分割点：<span class="hljs-subst">&#123;split_sentence&#125;</span>&quot;</span>)<br>    logging.info(<span class="hljs-string">f&quot;分割后的内容：\n国内：<span class="hljs-subst">&#123;domestic_content[-<span class="hljs-number">100</span>:]&#125;</span>...\n国外：<span class="hljs-subst">&#123;foreign_content[:<span class="hljs-number">100</span>]&#125;</span>...&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&#x27;domestic_content&#x27;</span>: domestic_content,<br>        <span class="hljs-string">&#x27;foreign_content&#x27;</span>: foreign_content<br>    &#125;<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">save_to_csv</span>(<span class="hljs-params">data, filename</span>):<br>    df = pd.DataFrame([data])<br>    <span class="hljs-keyword">try</span>:<br>        df.to_csv(filename, index=<span class="hljs-literal">False</span>, encoding=<span class="hljs-string">&#x27;utf-8-sig&#x27;</span>, mode=<span class="hljs-string">&#x27;a&#x27;</span>, header=<span class="hljs-keyword">not</span> os.path.exists(filename))<br>        logging.info(<span class="hljs-string">f&quot;成功保存内容到 <span class="hljs-subst">&#123;filename&#125;</span>：<span class="hljs-subst">&#123;data[<span class="hljs-string">&#x27;date&#x27;</span>]&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        logging.error(<span class="hljs-string">f&quot;保存到 CSV 文件时出错: <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(e)&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_news_files</span>(<span class="hljs-params">input_folder, output_file</span>):<br>    txt_files = [f <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> os.listdir(input_folder) <span class="hljs-keyword">if</span> f.endswith(<span class="hljs-string">&#x27;.txt&#x27;</span>)]<br>    total_files = <span class="hljs-built_in">len</span>(txt_files)<br><br>    <span class="hljs-keyword">for</span> idx, filename <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(txt_files, <span class="hljs-number">1</span>):<br>        date = filename[:-<span class="hljs-number">4</span>]<br>        file_path = os.path.join(input_folder, filename)<br>        <br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> text_file:<br>                news_text = text_file.read()<br>                logging.debug(<span class="hljs-string">f&quot;文件 <span class="hljs-subst">&#123;filename&#125;</span> 的内容：<span class="hljs-subst">&#123;news_text[:<span class="hljs-number">300</span>]&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            logging.error(<span class="hljs-string">f&quot;读取文件 <span class="hljs-subst">&#123;filename&#125;</span> 时出错: <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(e)&#125;</span>&quot;</span>)<br>            <span class="hljs-keyword">continue</span><br>        <br>        logging.info(<span class="hljs-string">f&quot;开始处理 <span class="hljs-subst">&#123;date&#125;</span> 的新闻 (<span class="hljs-subst">&#123;idx&#125;</span>/<span class="hljs-subst">&#123;total_files&#125;</span>)&quot;</span>)<br>        <br>        start_time = time.time()<br>        result = <span class="hljs-literal">None</span><br>        max_attempts = <span class="hljs-number">5</span><br><br>        <span class="hljs-keyword">for</span> attempt <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, max_attempts + <span class="hljs-number">1</span>):<br>            split_sentence = find_split_point(news_text)<br>            <span class="hljs-keyword">if</span> split_sentence:<br>                result = split_news_content(news_text, split_sentence)<br>                <span class="hljs-keyword">if</span> result:<br>                    <span class="hljs-keyword">break</span><br>            logging.warning(<span class="hljs-string">f&quot;第 <span class="hljs-subst">&#123;attempt&#125;</span> 次尝试未成功，等待10秒后重试...&quot;</span>)<br>            time.sleep(<span class="hljs-number">10</span>)<br>        <br>        <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            logging.error(<span class="hljs-string">f&quot;在 <span class="hljs-subst">&#123;max_attempts&#125;</span> 次尝试后仍未能正确分割 <span class="hljs-subst">&#123;date&#125;</span> 的内容&quot;</span>)<br>            <span class="hljs-keyword">continue</span><br><br>        save_to_csv(&#123;<br>            <span class="hljs-string">&#x27;date&#x27;</span>: date, <br>            <span class="hljs-string">&#x27;domestic_content&#x27;</span>: result[<span class="hljs-string">&#x27;domestic_content&#x27;</span>], <br>            <span class="hljs-string">&#x27;foreign_content&#x27;</span>: result[<span class="hljs-string">&#x27;foreign_content&#x27;</span>]<br>        &#125;, output_file)<br>        <br>        end_time = time.time()<br>        processing_time = end_time - start_time<br>        logging.info(<span class="hljs-string">f&quot;处理完成并保存: <span class="hljs-subst">&#123;date&#125;</span>，用时 <span class="hljs-subst">&#123;processing_time:<span class="hljs-number">.2</span>f&#125;</span> 秒&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    input_folder = <span class="hljs-string">&#x27;文本合集&#x27;</span>  <span class="hljs-comment"># 存放新闻文本文件的文件夹</span><br>    output_file = <span class="hljs-string">f&#x27;news_content_split_<span class="hljs-subst">&#123;datetime.now().strftime(<span class="hljs-string">&quot;%Y%m%d_%H%M%S&quot;</span>)&#125;</span>.csv&#x27;</span><br>    <br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(input_folder):<br>        logging.error(<span class="hljs-string">f&quot;输入文件夹 <span class="hljs-subst">&#123;input_folder&#125;</span> 不存在&quot;</span>)<br>        <span class="hljs-keyword">return</span><br><br>    start_time = time.time()<br>    process_news_files(input_folder, output_file)<br>    end_time = time.time()<br>    <br>    total_time = end_time - start_time<br>    logging.info(<span class="hljs-string">f&quot;处理完成，总用时 <span class="hljs-subst">&#123;total_time:<span class="hljs-number">.2</span>f&#125;</span> 秒&quot;</span>)<br>    <br>    <span class="hljs-keyword">if</span> os.path.exists(output_file):<br>        file_size = os.path.getsize(output_file)<br>        logging.info(<span class="hljs-string">f&quot;新闻内容已保存到 <span class="hljs-subst">&#123;output_file&#125;</span>，文件大小：<span class="hljs-subst">&#123;file_size&#125;</span> 字节&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        logging.error(<span class="hljs-string">f&quot;无法找到输出文件 <span class="hljs-subst">&#123;output_file&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure>
<p>这里的代码是我用Claude3.5 sonnet生成的，效果不错！</p>
<p><strong>本地模型 vs 云端模型</strong></p>
<p>在实验中，一开始用的是本地基于<strong>Ollama</strong>部署的<strong>Qwen
2.5 14b</strong>，结果效果很不理想，主要有两个障碍：</p>
<ul>
<li>14b模型本身还是太小了，在指令跟随、输出准确性方面，均有很大的不足；</li>
<li>其次是模型的速度太慢了，我的机器是2021款Macbook
Pro，M1pro芯片，16G内存。14b的模型能本地跑起来，用来聊天还行，但是批量处理文本的话，速度太慢，一晚上也只处理了几百条。</li>
</ul>
<p>发现本地模型的局限后，我选择了充钱，是的，充钱真的可以变强。调用了阿里云的<strong>Qwen
plus</strong>，并做了并发，结果又快又好，整个任务在半小时内处理完，API费用在10元左右。</p>
<p>在此之前我还使用过别人论文里的数据，简单比对过AI分类与传统机器学习分类的效果，在简单任务上几乎无差异，在复杂任务上，我也很难看出别人训练集里人类专家打的标签比AI更准确。</p>
<p><strong>可解释性问题</strong></p>
<p>与传统的机器学习方法一样，基于大模型的分类器也面临可解释性问题，甚至黑箱的问题更严重。但这并不妨碍其又快又好。我看到的文献里，对于这一部分的解释/辩驳通常如下：</p>
<ul>
<li>使用开源模型，如Llama 3.1系列模型，以方便复现；</li>
<li>对于闭源模型，详细阐述温度等参数、提示词、当前版本；</li>
<li>大方承认局限性。</li>
</ul>
<p>对于LLM，我的整体感受是，以前需要自己干的或者招RA干的脏活苦活累活，都可以用LLM试试，构建一套好用的工作流。当然用AI也有其局限性，包括费用、响应时长等。</p>
<h4 id="case-2-不知疲倦的学术同行supervisor-level"><strong>Case 2:
不知疲倦的学术同行（Supervisor level）</strong></h4>
<p>电视剧<strong>《西部世界》</strong>讲述了一个人与AI的故事：以为自己是人类的工程师阿诺德，通过“<strong>话疗</strong>”的方式不断启发仿生人德洛丽丝，阿诺德探寻着仿生人如何产生意识的问题，扮演者上帝的角色。在故事的后来我们才得知，原来德洛丽丝才是较早觉醒的仿生人，而阿诺德，其实是未觉醒的AI，在话疗里真正被启发的，是阿诺德。</p>
<p>前沿的大语言模型是个很好的“话疗”式启发工具，它永远在线、不会发脾气、<strong>不会pua你</strong>、不会说你的idea没有价值，它的知识比地球上任何人都多，它什么都可以聊，是不错的跨学科伙伴/导师。</p>
<p>当我脑中有一个<strong>火花</strong>时，我很喜欢和AI将这个火花聊下去，变成<strong>小火苗</strong>然后保存下来，这样，一个idea
就不再只是记事本里的一个条目。通常的流程如下：</p>
<ul>
<li>将我的idea告诉AI，请它帮我完善，进行多轮对话，逐步深入；</li>
<li>将对话的内容导出PDF，保存在work in progress的文件夹里（或者放去GPT
的projects里）。</li>
</ul>
<p>下面是一个实例演示，基本idea来源于我的研究领域和个人兴趣，我的方向是住房政治，我想了解的是<strong>女权主义的兴起与女性房主比例</strong>的关系，女权主义的兴起会使得女生们更有买房的动力吗？</p>
<p>与<strong>o1pro</strong>模型的对话：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220132337597.png" srcset="/img/loading.gif" lazyload />
<img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220132431381.png" srcset="/img/loading.gif" lazyload />
<img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220132455880.png" srcset="/img/loading.gif" lazyload />
<img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220132547166.png" srcset="/img/loading.gif" lazyload /></p>
<hr />
<p>在这个案例中，它给出了一个准实验设计，具有一定的参考性，当然这个设计距离Top5有非常大的距离。后面，我又与它就如何更好地测量<strong>女权主义扩散</strong>展开了更多的讨论，限于篇幅，就不一一展示。</p>
<p>与AI对话的好处是，它不排斥话题，还能给出建设性意见，帮助我完善，是一位很好的<strong>启发者</strong>。</p>
<p>（对于这个案例，如果您有很好的测量方法，能展现女权主义在中国扩散的variation，也请不吝联系我，谢谢。）</p>
<p>类似的案例还有许多，我们可以养成习惯，<strong>多问</strong>真的很重要。</p>
<p>在ChatGPT网页版导出PDF的插件是“<strong>ChatGPT to
PDF</strong>”，可以在应用商店安装。</p>
<h4 id="case-3-模型撰写与算法设计domain-expert-level"><strong>Case 3:
模型撰写与算法设计（Domain expert level）</strong></h4>
<p><strong>模型撰写</strong></p>
<p>我不是经济学专业的学生，连三高都没有学过，因此这部分内容我有用AI尝试，但是我没有判断能力，一下展示一个案例，供大家判断。
中文提示词如下（感谢安然姐提供）：</p>
<blockquote>
<p>建立了一个分析城乡人口迁移的数学模型。该模型应考虑城市和农村地区的经济机会、生活成本、就业率和生活质量等因素。它还应该考虑到影响个人迁移决定的推-拉因素。用微分方程来表示城市和农村地区随着时间的推移的人口流动。纳入关于迁移成本和返回迁移潜力的假设。利用现有的人口数据验证模型，并讨论其对城市化政策和区域发展战略的影响。</p>
</blockquote>
<p>英文原版提示词如下：</p>
<blockquote>
<p>Develop a mathematical model to analyze urban - rural population
migration. The model should consider factors such as economic
opportunities, cost of living, employment rates, and quality of life in
both urban and rural areas. It should also account for the push - pull
factors that influence individuals' decisions to migrate. Use
differential equations to represent the population flows between urban
and rural regions over time. Incorporate assumptions about migration
costs and the potential for return migration. Validate the model using
available demographic data and discuss its implications for urbanization
policies and regional development strategies.</p>
</blockquote>
<p>模型回答如下，思考时间为2m4s：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220141216918.png" srcset="/img/loading.gif" lazyload /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220141255666.png" srcset="/img/loading.gif" lazyload /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220141325579.png" srcset="/img/loading.gif" lazyload /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220141344412.png" srcset="/img/loading.gif" lazyload /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220141425176.png" srcset="/img/loading.gif" lazyload /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220141445397.png" srcset="/img/loading.gif" lazyload /></p>
<p>⬆️以上回答的效果由大家判断。</p>
<p>普遍的意见是，模型第一次给出的回答算是toy
model，离发表还有很大距离，需要使用者不断完善。</p>
<p><strong>算法设计</strong></p>
<p>这部分对我来说主要是写代码的工作，python、stata、R、typst等。我日常会在vscode里完成代码工作。下面是一个工作流示例：</p>
<ul>
<li>在vscode里配置<strong>stata</strong>的使用插件，比stata本身的do
file编辑器强一万倍，可以自动补全、高亮、实时保存，多版本编辑等等；</li>
<li>使用学生身份开通<strong>GitHub
copilot</strong>，实现AI代码补全与生成。</li>
</ul>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220135811022.png" srcset="/img/loading.gif" lazyload /></p>
<p>⬆️上面是一个示例，在vscode里编辑写do
file，在实际工作中，copilot可以帮助我干很多补全的工作，比如重命名变量、写循环、写回归代码，等等。</p>
<p>以上<strong>配置教程</strong>参见荔哥的文章：<a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/bZePPcsQL1Jw9C6mEF9Tdg">让免费AI做你的RA
-- STATA + VSCODE + Copilot</a></p>
<p>另外一个非常推荐的AI开发工具是<strong>cursor</strong>，在寒假前，由于常用的番茄时钟变得越来越难用，我尝试在cursor里开发了一个番茄时钟原型，能实现对一个项目不同阶段耗时的记录，界面如下：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220140401689.png" srcset="/img/loading.gif" lazyload /></p>
<p>总之，在AI的加持下，代码小白和新人，完全可以完成一些以前很难做到的任务。</p>
<h4 id="case-4-模拟人类agent-level"><strong>Case 4: 模拟人类（Agent
level）</strong></h4>
<p>假设AI在默认参数下是一个“<strong>平均人</strong>”，那么调整它的参数和给定背景，海量agent的情况下是不是就可以模拟一个<strong>社会</strong>？<strong>AI
as
agent</strong>可能是目前最火的社科AI项目。我没有开展过相关研究，在这里附上一些信息，包括有趣的项目和我写过的推文，供大家参考。</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/S84X5njED1wqqBl11oZIqg">AgentSociety
1.0：LLM时代的社会模拟器来了！</a>
其介绍是：“清华大学推出基于大模型的社会模拟器AgentSociety
1.0，可精确模拟社会舆论传播、认知观点极化、公众政策响应等社会现象，推动智能社会治理和科学研究范式变革。</li>
<li><a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/3Ttdc8_sxPTyt3zYyzR0KA">PNAS-人工智能可以提供情绪价值，但人们知道真相后效果下降</a>
利用心理学实验，这项研究发现AI生成的信息比人类生成的信息更能让接收者感到被倾听，但是当人们意识到消息来自AI时，人们感觉到自己被倾听的程度下降了。</li>
<li><a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/7gsnci46P2PMXvctHXIrVw">Science-赛博蜻蜓队长？AI在民主议事中的作用</a>
这篇论文探讨了人工智能（AI）在民主讨论中促进共识的潜力，通过开发一个名为“哈贝马斯机器”的AI调解系统，帮助人们在复杂的社会和政治问题上达成共同立场。通过一系列实验，包括涉及超过5000名参与者的虚拟公民议会，研究表明AI调解比人工调解更有效，生成的群体声明更加清晰、公正，并能减少组内分歧。尽管AI调解展现了出色的潜力，研究也提醒在确保讨论的公平性和代表性时应谨慎。总之，AI调解为促进大规模民主讨论提供了一个高效且可扩展的解决方案。</li>
<li><a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/FZic4im4meN7sAmII8RYRw">Science-与AI对话能持续减少阴谋论</a>
这篇论文提出了一种新颖且有希望减少阴谋论信念的方法，显示通过与AI的个性化对话，即使是深信不疑的人也可能改变他们的信念。这一方法不同于以往悲观的观点，认为深陷阴谋论的人几乎无法改变。研究表明，<strong>设计得当的AI系统可以在改善公共讨论和应对错误信息方面发挥重要作用</strong>，前提是它们能够被负责任地使用。</li>
</ul>
<p>再介绍一些AI agent协作平台或科研组织：</p>
<ul>
<li><strong>Expected Parrot</strong>：其开发了一系列AI
agent套件，可以方便地设定agent参数和部署，以方便开展基于agent
的模拟调查试验；</li>
<li><strong>Sakana.ai</strong>：这是一个AI科学家项目，创始人为Attention
Is All You
Need的作者之一，目的是建立基于大模型的全自动科学发现系统。最近的工作参见集智的报道：<a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/mMwPrnNXCh-QqMEPWdJ0XQ">《自动搜索人工生命》到底在搜索什么</a></li>
</ul>
<p>这部分内容值得开一期专题！</p>
<h4 id="case-5-宏观影响governance-level"><strong>Case 5:
宏观影响（Governance level）</strong></h4>
<p>这部分不算AI的应用，而算AI的影响，来看一篇有OpenAI发布的研究吧：</p>
<p>摘要翻译如下：</p>
<blockquote>
<p>我们提出了一个框架，用于评估大型语言模型（LLMs）及相关技术对工作的潜在影响。这个框架通过考虑这些技术与员工在工作中执行的任务的相关性来实现评估。通过应用这个框架（既使用了人工评估，也使用了LLM评估），我们估计，约有1.8%的工作岗位中，超过一半的任务可能会受到具有简单界面和通用训练的LLM的影响。当考虑到当前和未来可能出现的、能够补充LLM能力的软件发展时，这个比例跃升至略高于46%的工作岗位。</p>
</blockquote>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220143541239.png" srcset="/img/loading.gif" lazyload /></p>
<h4 id="case-6-可信文献综述生成与处理ra-level"><strong>Case 6:
可信文献综述生成与处理（RA level）</strong></h4>
<p>这部分长期以来是广大科研人员的痛点、难点，早期<strong>AI幻觉</strong>很严重，非常容易编造文献。但随着AI的迅猛发展，这一难点正在被逐步攻克。我在去年的推文中介绍其中的一些方法，参见：<a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Y6N1lEITz02elLy1Tk2Hsw">介绍两个AI文献综述平台</a>。</p>
<p>最近两个月，我有了一些新的发现，跟大家分享。</p>
<p>首先整理了一些AI文献综述网站/平台：</p>
<ul>
<li><strong>Paper
Digest</strong>：https://www.paperdigest.org/review/</li>
<li><strong>scite</strong>：https://scite.ai/</li>
<li><strong>Consensus</strong>：https://consensus.app/search/</li>
<li><strong>Ai2 Scholar QA</strong>：https://scholarqa.allen.ai/</li>
<li><strong>Elicit</strong>：https://elicit.com/</li>
<li><strong>ScienceDirect
AI</strong>：https://www.sciencedirect.com/ai</li>
</ul>
<p>它们的好处是，都有<strong>论点支持的文献搜索</strong>能力，即让AI返回支持xx论点的文献，这些文献是真实存在的。在pw等好友的使用反馈里，Consensus是比较优秀的，我个人认为Ai2
Scholar QA和ScienceDirect AI也不错。</p>
<p>还有一种新的<strong>大力出奇迹</strong>方法，整理好几十篇同主题论文的标题、作者姓名、年份、摘要和主要内容，一股脑塞给支持<strong>长上下文</strong>的模型，比如o1pro或Gemini
2.0系列，能得到一个相当不错的结果，逻辑清晰、引用准确。比如下面，我整理了几十篇关于“<strong>中国官员晋升模式</strong>”文献的bibtex，删去无效字段后丢给o1pro让它写综述，不说的话你能分辨出是AI生成的吗？⬇️</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220150454669.png" srcset="/img/loading.gif" lazyload /></p>
<p>在<strong>PA领域耀眼Star潘王</strong>那里学到一个AI辅助文献的新工作流，适用于使用<strong>latex</strong>或者<strong>typst</strong>工作的同学，即要求模型以支持latex或者typst引文的方式输出，这样输出的内容就直接有了完善的引用，不需要后期手动调整。这避免在word里手动将AI生成的内容改成zotero等文献管理软件插入的麻烦，大致效果如下：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/yuzhangnju/image2025/image-20250220151034216.png" srcset="/img/loading.gif" lazyload /></p>
<p>⬆️图中，左侧是typst文件，文字由AI生成，右侧是对应的pdf，右侧的蓝色部分表示成功的引用。</p>
<h4 id="case-7-deep-researchsuper-agent-level"><strong>Case 7: deep
research（Super agent level）</strong></h4>
<p>它可能是你我失业的原因，<strong>伟大无需多言</strong>。</p>
<p>在2023年，我想的是AI对我有什么价值，在2024年，我会想我对AI有什么价值，现在我会想，晚上吃什么比较好。AI在过去两年的进步速度太快了，按照OpenAI自己的路线图，L3级的AI，即agent已经在落地。Deep
research的一个案例参考下面的推文：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/cSFyFFnp5WVxmjYcrD3vtw">Deep
research将使文献分享类公众号失去意义</a></li>
</ul>
<p>以下是OpenAI deep research的一些技术特征（由perplexity给出）：</p>
<blockquote>
<p>OpenAI的Deep
Research是一个新型的AI深度研究工具，旨在为需要进行复杂和深入研究的用户提供支持。该工具能够在短时间内（通常为5到30分钟）完成原本需要人类专家数小时甚至数天才能完成的研究任务。其特点有：</p>
<ul>
<li><strong>多步骤自主研究</strong>：用户仅需提供一个提示，Deep
Research便能自动搜索、分析和整合来自互联网上的大量信息，生成专业级的研究报告。</li>
<li><strong>基于o3模型</strong>：Deep
Research建立在OpenAI最新推出的o3模型之上，该模型经过优化，专门用于网页浏览和数据分析。它具备强大的推理能力，可以根据实时信息调整研究方向。</li>
</ul>
</blockquote>
<p>这些天里，我尝试用deep
research生成了一些报告和综述，整体<strong>感受</strong>如下：</p>
<ul>
<li>deep
research生成的内容相当可用，其工作一次给出的结果，我可能需要干一个月；</li>
<li>deep
research擅长生成报告类内容，暂时不适合别的内容，因为其被设计为一个科研agent，干别的内容它不太会遵循指令；</li>
<li>使用过程中，单篇报告长度可达6万字，通常在2万左右。</li>
</ul>
<p>可以想象deep
research对于一些<strong>产出为纯文字</strong>的科研人员冲击很大，包括文史哲等文科专业，还有擅长写空话的管理学等专业，它会使得本就需要堆字数的<strong>横向课题</strong>更加没有意义。</p>
<p>目前deep
research撰写综述的难点是期刊付费墙，一旦打通期刊库，传统文献综述的意义在哪里？</p>
<p>Google 和 perplexity也有自己的deep
research功能，但目前的效果和OpenAI的相距甚远。</p>
<h4 id="其他工具"><strong>其他工具</strong></h4>
<ul>
<li>门户：https://ai-bot.cn/</li>
</ul>
<p><strong>国产研究平台</strong></p>
<ul>
<li>秘塔搜索：https://metaso.cn/</li>
<li>天工AI：https://www.tiangong.cn/</li>
</ul>
<p><strong>AI写作纠错工具</strong></p>
<ul>
<li>科大讯飞-飞鹰智能校对系统：https://feiying.iflytek.com/</li>
<li>秘塔写作喵：https://xiezuocat.com/</li>
<li>Worddvice.AI：https://wordvice.ai/cn</li>
<li>Writefull：https://x.writefull.com/</li>
<li>QuillBot：https://quillbot.com/</li>
<li>讯飞公文写作：https://gw.iflydocs.com/?from=aibot#/dashboard</li>
<li>Microsoft 365
Copilot：https://www.microsoft.com/en-us/microsoft-365/copilot</li>
</ul>
<h2 id="号外">号外</h2>
<ul>
<li>推文写到案例部分非常难受，因为这些案例需要<strong>实时演示</strong>，截图很难有好的效果，而且太多截图真的很不美观。</li>
<li>我还用deep
research生成了几个版本的“<strong>AI如何加速学术研究</strong>”，放在链接里，感兴趣的老师同学欢迎下载查看，我认为其质量会让你感到惊喜。如果我是<strong>学阀</strong>，里面的一些内容完全可以通过“<strong>微信投稿</strong>”给C刊编辑。另外，与本文提到的5个层次对应的文献，和我pre时用的ppt，一同在链接里。（链接:
https://pan.baidu.com/s/1BUmy5m6WUy6V66IKM7rLqQ?pwd=fqck 提取码: fqck
）</li>
<li><strong>API is all you need</strong>. 这个月deepseek
r1爆火，官网和app挤不进去，大家或许已经在尝试使用云厂商的API服务。许多时候调用API，真的可以大力出奇迹，相当于找了一个RA，用很直接的、第一性的方式解决问题。这里顺势推荐一下<strong>cherry
studio</strong>，我之前配置云端模型一般用OpenWeb UI，最近发现cherry
studio更好用，配置教程参见波哥的推文：<a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/DYoC9wqeEMyoyP81s7GXuA">如何白嫖各大巨头的DeepSeek额度</a></li>
<li>如果您有好用的AI案例，也请给我留言，非常感谢。</li>
<li>或许在未来不久，大家都会经历自己的“<strong>李世石时刻</strong>”，在认为是自己专业的领域，被AI赶超。所以，在这之前，多了解了解AI吧。</li>
<li>如果您的课题组不是极度缺经费，我建议开通前沿模型使用一下，至少给课题组的一个成员开通。由于这个费用不好报销，以劳务费或者助研费资助是个不错的选择。</li>
<li>如果您的课题组对本文的内容感兴趣，想进一步交流，还请直接私信联系。</li>
<li>如果您觉得本文对您有所帮助，还请不吝在下方⬇️对我的工作进行“<strong>赞赏</strong>”，以帮助我减轻大模型的使用费压力，这部分我是在自费工作，非常感谢！（B站up主要“三连”即视感😂）</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/HIBL/" class="print-no-link">#HIBL</a>
      
        <a href="/tags/AAAR/" class="print-no-link">#AAAR</a>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>AAAR-使用AI加速学术研究的方法论与经验</div>
      <div>https://yuzhang.net/2025/02/20/20250220-AAAR/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Yu Zhang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年2月20日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="NC - 非商业性使用">
                    <i class="iconfont icon-nc"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/02/24/20250224-ECTA-%E6%AC%A7%E6%B4%B2%E5%90%9B%E4%B8%BB%E5%AF%B9%E5%9B%BD%E5%AE%B6%E7%BB%A9%E6%95%88%E7%9A%84%E5%BD%B1%E5%93%8D/" title="ECTA-欧洲君主对国家绩效的影响">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">ECTA-欧洲君主对国家绩效的影响</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/02/18/20250218-Deep%20research%E5%B0%86%E4%BD%BF%E6%96%87%E7%8C%AE%E5%88%86%E4%BA%AB%E7%B1%BB%E5%85%AC%E4%BC%97%E5%8F%B7%E5%A4%B1%E5%8E%BB%E6%84%8F%E4%B9%89/" title="Deep research将使文献分享类公众号失去意义">
                        <span class="hidden-mobile">Deep research将使文献分享类公众号失去意义</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  




  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
